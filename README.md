# Integraci贸n de Ollama + Deepseek con Python + Fastapi

Este proyecto proporciona una API REST construida con FastAPI que se conecta a un modelo local de IA (deepseek-r1) gestionado por Ollama. Permite enviar consultas en lenguaje natural y recibir respuestas generadas por el modelo.

## Prerrequisitos

- Python 3.13 instalado y accesible en $PATH para facilitar el uso de Python desde la terminal [Descargar Python 3.13](https://www.python.org/downloads/release/python-3130/)
- Ollama instalado y funcionando en localhost [Descargar Ollama](https://ollama.com/download)
- Modelo `deepseek-r1` descargado con Ollama
  ```shell
  ollama pull deepseek-r1
  ```
- Postman instalado y crear una cuenta gratuita para acceder a todas las funcionalidades de la herramienta [Descargar Postman](https://www.postman.com/downloads/)

## Comenzar

- Ir a https://ollama.com
- Instalar Ollama en el sistema
- Extraer modelo deepseek-r1
- S铆rvelo como servidor en el puerto: `11434`
- Ir a `https://github.com/ollama/ollama?tab=readme-ov-file`


## Estructura del proyecto
```bash
 app/                   # Carpeta principal del c贸digo
     main.py            # Archivo principal para ejecutar la aplicaci贸n
     config.py          # Configuraci贸n y carga de .env
     schemas.py         # Definici贸n de modelos de datos (Pydantic)
     exceptions.py      # Manejadores de errores personalizados
     services/
         ollama.py      # L贸gica de comunicaci贸n con Ollama
 .env                   # Archivo para gestionar variables de entorno
 requirements.txt       # Archivo para las dependencias del proyecto
 README.md              # Documento explicativo del proyecto
```

## Instalaci贸n

Clonar el repositorio

```shell
git clone semana0_rafael_callata
cd semana0_rafael_callata
```

Abra su terminal y cree un entorno virtual con el siguiente comando.
```shell
python3 -m venv ./.venv
.\env\Scr\activate            # Windows (CMD/Powershell)
#source ./env/bin/activate    # macOS / Linux
```

A continuaci贸n, instale los siguientes paquetes y dependencias para el correcto funcionamiento del proyecto.
```shell
pip install --no-cache-dir -r requirements.txt
```
Configurar variables de entorno, cree un achivo `.env` con los siguientes variables dentro del archivo:
```shell
OLLAMA_HOST=http://127.0.0.1
OLLAMA_PORT=11434
OLLAMA_MODEL=deepseekr1
```

Verificar el modelo Ollama este funcionando correctamente en tu sistema operativo
```shell
ollama list
# debe aparecer: deepseekr1
```

## Ejecutar API
Para ejecutar la aplicaci贸n en modo desarrollo con autoreload, escriba el siguiente comando.
```shell
uvicorn app.main:app --reload --host=<host> --port=<port>
```
Por ejemplo,
```shell
uvicorn server:app --reload --host=127.0.0.1 --port=8000
```
A continuaci贸n, visite: `http://127.0.0.1:8000/docs`

### Endpoints

- GET /
  - Descripci贸n: Verifica el estado de la API.
  - Respuesta: { "status": "ok" }
- POST /preguntar
  - Descripci贸n: Env铆a una pregunta al modelo.
  - Body (JSON):
    ```shell
    {
      "prompt": "驴Cu谩l es la capital de Francia?",
      "stream": false,
      "temperature": 0.7,
      "max_tokens": 512
    }
    ```
  - Respuesta (JSON):
    ```shell
    { "respuesta": "Par铆s" }
    ```

## Prueba del API
Antes de probar el paquete de instalaci贸n de la API con el siguiente comando.requests
```shell
pip install requests
```
### Pruebas con Postman
1. Importa o crea una colecci贸n en Postman.

2. Configura una solicitud POST http://localhost:8000/preguntar.

3. En el Body, selecciona raw JSON y pega un ejemplo de payload.

4. Env铆a la petici贸n y verifica la respuesta.

5. Exporta la colecci贸n con capturas de los resultados.
El usuario pregunt贸:
```shell
{
  "prompt": "驴Cu谩l es la capital de Francia?"
}
```
Respuesta de salida de la API:
```shell
{
    "respuesta":"{\"model\":\"deepseek-r1\",\"created_at\":\"2025-04-19T02:28:48.5405089Z\",\"response\":\"\<think\>\\n\\n\</think\>\\n\\nLa capital de Francia es Par铆s.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[151644,30182,44819,19003,1531,1187,6722,409,9694,685,30,151645,151648,271,151649,271,8747,6722,409,9694,685,1531,4270,23422,13],\"total_duration\":3763040100,\"load_duration\":19288200,\"prompt_eval_count\":13,\"prompt_eval_duration\":260436600,\"eval_count\":14,\"eval_duration\":3482755300}"
}
```
## Investigaci贸n

**1. 驴Qu茅 es Ollama?**

   Ollama es una herramienta de l铆nea de comandos que facilita la descarga y ejecuci贸n de modelos de inteligencia artificial localmente. Proporciona una API REST (por defecto en el puerto 11434) para enviar prompts y recibir respuestas sin necesidad de servicios en la nube.
   
**2. 驴Qu茅 es FastAPI?**
   
   FastAPI es un framework moderno de Python para construir APIs web de alto rendimiento. Se basa en Starlette y Pydantic, ofrece validaci贸n autom谩tica de datos, generaci贸n de documentaci贸n OpenAPI (Swagger) y un rendimiento cercano al de Node.js o Go.
   
**3. 驴Qu茅 es el modelo deepseek-r1?**

   Deepseek-r1 es un modelo de lenguaje optimizado para tareas de procesamiento de texto. Est谩 dise帽ado para equilibrar precisi贸n y eficiencia, permitiendo implementaciones locales con recursos moderados.
   
**4. Uso de peticiones con stream=True**

   Al habilitar stream=True, la API de Ollama env铆a fragmentos de la respuesta a medida que se generan. Esto permite construir interfaces interactivas o pipelines m谩s responsivos, mostrando tokens en tiempo real.
      
**5. 驴C贸mo garantizar la escalabilidad de una API que consume modelos de IA pesados?**
   
   - Desplegar m煤ltiples r茅plicas de Ollama y balancearlas con un proxy (NGINX, Traefik).
   - Implementar colas de trabajo (RabbitMQ, Redis) para encolar peticiones.
   - Usar caching en nivel de petici贸n (Redis) para respuestas repetidas.
   - Limitar concurrencia y tiempo de ejecuci贸n con sem谩foros o middleware.

**6. 驴Qu茅 par谩metros de Ollama (ej: num_ctx, temperature) afectan el rendimiento/calidad de respuestas?**
   
   - **temperature:** controla la aleatoriedad; valores m谩s bajos generan respuestas m谩s deterministas.
   - **max_tokens:** define la longitud m谩xima de la respuesta.
   - **num_ctx o context_size:** ajusta cu谩ntos tokens del prompt se retienen en memoria, afectando latencia y uso de RAM.
     
**7. 驴Qu茅 estrategias usar para balancear carga entre m煤ltiples instancias de Ollama?**
   
   - Usar un balanceador HTTP (NGINX, HAProxy) que distribuya peticiones round-robin.
   - Implementar un gateway que monitorice la salud y carga de cada instancia.
   - Autoescalar instancias Docker/Kubernetes seg煤n m茅tricas de CPU/latencia.
  
**8. 驴Qu茅 patrones de dise帽o (ej: CQRS, Singleton) son 煤tiles para integrar modelos de IA en backend?**
    
  - **Singleton:** para que la configuraci贸n de cliente HTTP o conexi贸n a Ollama se inicialice una sola vez.
  - **Factory:** para crear servicios de IA que puedan apuntar a distintos modelos o proveedores.
  - **CQRS (Command Query Responsibility Segregation):** separar rutas de lectura (predicciones) y escritura (registro de uso) para optimizar y escalar cada parte.

